
<html>
  <head>
    <meta charset="UTF-8">
    <title>Workshop - Mathematics of data streams</title>
    <style>
      @import url("https://cdn.jsdelivr.net/npm/bootstrap-icons@1.3.0/font/bootstrap-icons.css");
      @import url('https://fonts.googleapis.com/css2?family=Roboto+Mono:wght@400;700&display=swap');
      body {
        font-family: 'Roboto Mono',  monospace;
        font-size: 11pt;
      }
      table {
        margin: 0;
        padding: 0;
        width: 100%;
        font-size: 100%;
        table-layout: fixed;
        border-collapse: collapse;
      }
      h2 small {
        font-weight: normal;
        font-size: 70%;
      }
      td, th {
        padding: 0.5em;
        text-align: center;
      }
      td {
        border: 1px solid;
      }
      .container {
        display: flex;
        width: 70%;
        margin: 0 auto;
        flex-flow: row wrap;
      }
      .content {
        width: 100%;
      }
      .panel {
        flex: 1 0 100%;
      }
      .speakers {
        display: flex;
        flex-flow: row wrap;
        justify-content: space-evenly;
      }
      .right {
        margin-left: 1em;
      }
      .left {
        margin-right: 1em;
      }
      .schedule {
        display: flex;
        flex-flow: column;
        flex: 4 0 auto;
      }
      .logos {
        text-align: center;
        padding-top: 0px;
      }
      .logo {
        /* height: 80px;
        width: auto; */
        width: 20%;
        height: auto;
        margin: 0 1em;
        vertical-align: middle;
        display: inline-block;
      }
      @media (max-width:1414px) {
        .panel {
          flex: 1 0 100%;
        }
        .left, .right {
          margin: 0;
        }
        .logos {
          margin: 1em 0em;
          /* order: 1; */
        }
        .schedule {
          order: 2
        }
        .container{
          width: 95%
        }
      }
      td.time {
        font-weight: bold;
      }
      tr.break {
        background: whitesmoke;
      }
      td.break {
        text-align: center;
        font-weight: bold;
      }
      td.coffee {
        background: whitesmoke;
      }
      td.keynote {
        background: aliceblue;
      }
      td.springschool {
        background: rgb(234, 250, 230)
      }
      a {
        text-decoration: none;
      }
      a:visited {
        color: blue;
      }
      ul {
        padding-inline-start: 0;
      }
      ul li {
        list-style-type: none;
        padding: 0;
        margin: 0;
      }
      p {
        white-space: pre-wrap;
      }
    </style>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  </head>
  <body>
    <div class="container">
      <div class="content">
        <h1>Mathematics of data streams: signatures, neural differential equations, and diffusion models</h1>
        <h2 style="color: gray">8&mdash;9 April, 2024: spring school</h2>
        <h2>10&mdash;13 April, 2024: workshop</h2>
        <h2>Location: Greifswald, Germany</h2>
        <ul>
          <li>&#8226; Location of the spring school (ATTENTION: LOCATION CHANGED): University of Greifswald, Institute of Math. and Comp. Sci., <a href="https://math-inf.uni-greifswald.de/en/department/about-us/locations/">Franz-Mehring-Str. 47, Greifswald</a>, Seminarraum 401 (Room 401)</li>
          <li>&#8226; Location of the workshop: <a href="https://www.wiko-greifswald.de/en/directions/">Alfried Krupp Kolleg</a></li>
        </ul>
        <p>We kindly ask participants to fill the following: <a href="https://docs.google.com/forms/d/1uvFD05EkVXknVoeTfzKiZIY2aZGzt3KpRSebM_HPAxE/">registration form</a>.</p>
        <p>Transportation and accommodation:</p>
        <ul>
            <li>&#8226; From abroad: airplane to Berlin, then Berlin to Greifswald via train. Train tickets can be bought on <a href="https://int.bahn.de/en">bahn.de</a> or via the DB Navigator App (Berlin to Greifswald is approx. 2:30 long).</li>
            <li>&#8226; From Greifswald train station to hotels, University and Alfried Krupp Kolleg: all places can be reached on foot.</li>
            <li>&#8226; Accommodation: Greifswald is a small city with limited number of hotels. Please try to book well in advance.</li>
            <li>&#8226; Funding requests: please see the form above.</li>
        </ul>
        <p>For any further inquiries: please reach out to Raphael Mignot (firstname.lastname at univ-lorraine.fr) and Leonard Schmitz (firstname . lastname at uni-greifswald.de).</p>
      </div>
      <div class="panel speakers">
        <div style="flex: 1 0 100%">
        <h1>List of speakers</h1>
        </div>
        <div style="flex: 1 0 25%;">
        <h3>Keynote speakers</h3>
        <ul>
          <li>Darrick Lee</li>
          <li>Harald Oberhauser</li>
          <li>Hao Ni</li>
          <li>Yuzuru Inahama</li>
          <li>Kenji Fukumizu</li>
          <li>Valentin de Bortoli</li>
          <li>Adeline Fermanian</li>
          <li>Oleksandr Shchur</li>
          <li>Mario Stanke</li>
        </ul>
        </div>
        <div style="flex: 2 0 auto; margin-right: 100px;">
        <h3 style="margin-bottom:0;">Invited speakers</h3>
        <ul style="float: left;">
          <li>Carlos Amendola</li>
          <li>Kurusch Ebrahimi-Fard</li>
          <li>Antonio Orvieto</li>
          <li>Remi Vaucher</li>
          <li>Maud Lemercier</li>
          <li>Linus Bleistein</li>
        </ul>
        <ul style="float:right;">
            <li>Nikolas Tapia</li>
            <li>Fabian Harang</li>
            <li>Tim Seynnaeve</li>
            <li>Marco Rauscher</li>
            <li>Giovanni Ballarin</li>
            <li>Jeremy Reizenstein</li>
        </ul>
        </div>
        <div style="flex: 1 0 25%;">
          <h3>Organizing Committee</h3>
          <ul>
            <li>Joscha Diehl</li>
            <li>Marianne Clausel</li>
            <li>Konstantin Usevich</li>
            <li>Nozomi Sugiura</li>
            <li>Stephane Chretien</li>
            <li>Leonard Schmitz</li>
            <li>Raphael Mignot</li>
          </ul>
        </div>
      </div>
      <div class="logos">
        <!-- <img class="logo" height="80" width="286" src="./img/DFG-logo.png"/>
        <img class="logo" height="80" width="272" src="./img/UG-logo.svg"/>
        <img class="logo" height="80" width="278" src="./img/WIKO-logo.svg"/> -->
        <img class="logo" src="./img/DFG-logo.png"/>
        <img class="logo" src="./img/UG-logo.svg"/>
        <img class="logo" src="./img/WIKO-logo.svg"/>
        <img class="logo" src="./img/SFDS-logo.png"/>
      </div>
      <div class="panel">
        <div class="schedule">
        <h1>Schedule</h1>
        All times CET (GMT+2)
        <div style="display: inline-block; position: relative;"><div style="display: inline-block; background-color: rgb(234, 250, 230); border: 1px solid black; height: 15px; width: 20px; position: absolute; top: 50%; transform: translateY(-50%);"></div><div style="display:inline-block; margin-left:2em;">spring school</div>, <div style="display: inline-block; background-color: aliceblue; border: 1px solid black; height: 15px; width: 20px; position: absolute; top: 50%; transform: translateY(-50%);"></div><div style="display:inline-block; margin-left:2em;">keynote</div>,  <div style="display: inline-block; background-color: whitesmoke; border: 1px solid black; height: 15px; width: 20px; position: absolute; top: 50%; transform: translateY(-50%);"></div><div style="display:inline-block; margin-left:2em;">break</div></div>
        <table>
          <tr><th></th><th>Monday 8</th><th>Tuesday 9</th><th>Wednesday 10</th><th>Thursday 11</th><th>Friday 12</th><th>Saturday 13</th></tr>
          <tr><th class="time">09:00&ndash;09:30</td><td rowspan="2" class="springschool">Diffusion models (de Bortoli)</td><td rowspan="2" class="springschool">Neural ODEs (Fermanian + Bleistein)</td><td rowspan="2" class="keynote">Plenary (Lee)</td><td rowspan="2" class="keynote">Plenary (Fermanian)</td><td rowspan="2" class="keynote">Plenary (Shchur)</td><td rowspan="2" class="keynote">Plenary (Stanke)</td></tr>
          <tr><th class="time">09:30&ndash;10:00</td></tr>
          <tr class="break"><th class="time">10:00&ndash;10:30</td><td colspan=6 class="break">Coffee Break</td></tr>
          <tr><th class="time">10:30&ndash;11:00</td><td rowspan="3" class="springschool">Diffusion models (de Bortoli)</td><td rowspan="3" class="springschool">Neural ODEs (Fermanian + Bleistein)</td><td>Talk (Lemercier)</td><td class="coffee">Break</td><td>Talk (Harang)</td><td class="coffee">Discussion + coffee</td></tr>
          <tr><th class="time">11:00&ndash;11:30</td><td>Talk (Amendola)</td><td>Talk (Orvieto)</td><td>Talk (Tapia)</td><td rowspan="2" class="keynote">Plenary (Ni)</td></tr>
          <tr><th class="time">11:30&ndash;12:00</td><td class="coffee">Lunch</td><td>Talk (Rauscher)</td><td>Talk (Ebrahimi-Fard)</td></tr>
          <tr class="break"><th class="time">12:00&ndash;13:00</td><td colspan=6 class="break">Lunch</td></tr>
          <tr><th class="time">13:00&ndash;13:30</td><td rowspan="6" class="springschool">Diffusion models (de Bortoli)</td><td rowspan="6" class="springschool">Neural ODEs (Fermanian + Bleistein)</td><td rowspan="3">Gong talks</td><td>Talk (Seynnaeve)</td><td rowspan="4">Applications afternoon (Chairman: Chretien) (with Vaucher, Ballarin, Sugiura)</td><td>End</td></tr>
          <tr><th class="time">13:30&ndash;14:00</td><td>Talk (Reizenstein)</td></tr>
          <tr><th class="time">14:00&ndash;14:30</td><td class="coffee">Break</td></tr>
          <tr><th class="time">14:30&ndash;15:00</td><td rowspan="3">Posters + coffee</td><td rowspan="3">Posters + coffee</td></tr>
          <tr><th class="time">15:00&ndash;15:30</td><td rowspan="4" class="coffee">Discussion + coffee</td></tr>
          <tr><th class="time">15:30&ndash;16:00</td></tr>
          <tr><th class="time">16:00&ndash;16:30</td><td rowspan="2" class="coffee">Discussion + coffee</td><td rowspan="2" class="coffee">Discussion + coffee</td><td rowspan="2" class="keynote">Plenary (Inahama)</td><td rowspan="2" class="keynote">Plenary (Fukumizu)</td></tr>
          <tr><th class="time">16:30&ndash;17:00</td></tr>
        </table>
        </div>
        Note: it is not a definitive schedule as some slots might be swapped in the future.
      </div>
      <div class="panel">
        <h1>Abstracts</h1>
        <h1>Keynotes</h1>
        <h2>Darrick Lee (University of Oxford)</h2>
        <h3>Random Surfaces and Higher Algebra (Part II)</h3>
        <p>Classical vector valued paths are widespread across pure and applied mathematics: from stochastic processes in probability to time series data in machine learning. Parallel transport (or path development) and path signatures provide an effective method to characterize such paths while preserving the concatenation structure of paths. In this talk, we extend this framework to build structure-preserving characterizations of surfaces using surface holonomy. This is based on joint work with Harald Oberhauser.</p>        
        <!--
        <h2>Harald Oberhauser (University of Oxford)</h2>
        <h3>Random Surfaces</h3>
        <p>Motivated by the success of the (expected) path signature in the study of stochastic processes that are indexed by a one-dimensional set, we develop a (expected) surface signature to study stochastic processes indexed by a two-dimensional set. The path signature captures the structure of (unparametrized) paths by turning path concatenation into group multiplication which ultimately allows to go from local to global descriptions. A key step is to find an appropriate generalization of a group which is rich enough to represent the different ways a surface can be glued together. Joint work with Darrick Lee.</p>
        -->
        <h2>Yuzuru Inahama (Kyushu University)</h2>
        <h3>Wong-Zakai approximation of density functions</h3>
        <p>We discuss the Wong-Zakai approximation of probability density functions of solutions (at a fixed time) of rough differential equations driven by fractional Brownian rough path with Hurst parameter H (1/2 >= H > 1/4). Besides rough path theory, we also use Hu-Watanabe's approximation theorem in the framework of Watanabe's distributional Malliavin calculus. When H=1/2, the random rough differential equations coincide with the corresponding Stratonovich-type stochastic differential equations. Even for that case, our main result seems new.</p>
        <h2>Hao Ni (University College London)</h2>
        <h3>Inverting the path signature from its unitary development</h3>
        <p>The signature of a path, as a core object in rough path theory, is a faithful transformation from the path space into the tensor algebra space. While the signature is regarded as non-commutative monomials on the path space, the development of the path under some Lie group can be viewed as the moment generating function of the signature. Both signature and development serve as the principled and effective feature representations for streamed data in machine learning. Using an algebraic approach, [Chevyrev and Lyons 2016] showed that the development under the unitary group can uniquely determine the signature. In this talk, we provide an alternative, novel and constructive proof that offers a method for the explicit inversion of the signature from the unitary development, analogous to deriving moments from a moment-generating function. Our approach not only can be applied to show the uniqueness of the development under some other Lie groups, but also leads to a novel,  general and computable metric on the probability measures on the path space. We illustrate the practical applications of our method through hypothesis testing on stochastic processes, highlighting its potential in generative models for time series generation.</p>
        <h2>Oleksandr Shchur (AWS AI)</h2>
        <h3>Chronos: Pretrained Models for Time Series Forecasting</h3>
        <p>Time series forecasting is an essential component of decision-making in domains such as energy, retail, and finance. Traditionally, machine learning practitioners have focused on developing task-specific forecasting models that are restricted to a certain dataset or application domain. Inspired by the success of pretrained Large Language Models (LLMs) in natural language processing, it becomes imperative to explore whether a similar approach can be applied to forecasting: Can we train a single large model on huge amounts of diverse time series data, that will generalize to new unseen time series tasks? In this talk, we introduce Chronos, a family of pretrained forecasting models based on minimal modifications to LLM architectures, that accomplishes this goal. Chronos demonstrates remarkable zero-shot performance on unseen datasets, positioning pretrained models as a viable tool to greatly simplify forecasting pipelines.</p>
        <h2>Adeline Fermanian (Califrais)</h2>
        <h3>Dynamic Survival Analysis with Controlled Latent States</h3>
        <p>We consider the task of learning individual-specific intensities of counting processes from a set of static variables and irregularly sampled time series. We introduce a novel modelization approach in which the intensity is the solution to a controlled differential equation. We first design a neural estimator by building on neural controlled differential equations. In a second time, we show that our model can be linearized in the signature space under sufficient regularity conditions, yielding a signature-based estimator which we call CoxSig. We provide theoretical learning guarantees for both estimators, before showcasing the performance of our models on a vast array of simulated and real-world datasets from finance, predictive maintenance and food supply chain management.</p>
        <h2>Kenji Fukumizu (Institute of Statistical Mathematics)</h2>
        <h3>Neural Fourier Transform: learning group representation from data</h3>
        <p>In this study, we introduce a novel deep learning framework designed to infer group representations from data under the assumption that the data space is subject to an unknown group action. The data consists of examples of the group action, comprising a point and its transformation under a group element, or sequences generated through the successive application of a group element. Utilizing an autoencoder architecture, our approach maps the data to a latent space in a manner that is equivariant to the group action, achieving linear group action on the latent variables and thus approximating a group representation. Further, by applying block-diagonalization, we approximately decompose the representation into irreducible representations. We call this method the Neural Fourier Transform. This presents a generalized, data-driven approach to Fourier transform. We validate our framework across various scenarios, including one notable case where the group and its actions are not explicitly known, yet our method successfully learns the equivariant mapping and a group representation using only sequential data. Employing image sequences altered by transformations affecting color or shape, we demonstrate that our derived irreducible representations effectively disentangle the underlying generative processes of the data. Theoretical results supporting our methodology are also presented.</p>
        <h2>Mario Stanke (University of Greifswald)</h2>
        <h3>Models for Evolution: Continuous-Time Markov Chains on a Tree</h3>
        <p>A standard model for the evolution of biological sequences is a continuous-time Markov chain on a discrete state space that evolves characters along the edges of a tree. Only the characters at the leaves are observed. The methods that are currently routinely applied in large scale to estimate the tree, its edge lengths and the parameters of the Markov chain are computationally intensive and unsatisfactory from a theoretical standpoint: The gradient of the likelihood with respect to the rate parameters of the process are estimated numerically and the tree is optimized by making random `moves` in the discrete space of all trees. I will present an auto-differentiable machine-learning layer that computes the likelihood of given sequence data. We have used this layer to discriminatively train multiple rate matrices end-to-end with a recurrent neural network for the classification of input sequences as either coding for a protein or not. We are currently using this to find genes in 64 mammalian genomes, leveraging thereby the fact that characters that code for a protein are under a particular evolutionary pressure. While this layer can also be used to compute the gradient with respect to the branch lengths, it is to my knowledge an open but worthwhile challenge to embed the discrete tree space such that in a joint model of tree, branch lengths and rate matrices the gradient with respect to all parameters of evolution can be computed and thus phylogenetic reconstruction or evolutionary classification be sped up or improved.</p>
        <h1>Invited talks</h1>
        <h2>Fabian Harang (BI Norwegian Business School)</h2>
        <h3>On the Signature of an Image</h3>
        <p>An analytic and algebraic understanding of iterated integral signatures associated to continuous paths has played a central role in in a wide range of mathematical areas, such as the construction of stochastic integration for non-martingales with rough paths theory, to formal representations and expansions of solutions to (partial) differential equations. In recent years, the signature has proven to be an efficient feature map for machine learning tasks, where the learning task is related to time series data, or data streams. In contrast to time series data, image data can naturally be seen as two-parameter fields taking values in multi-dimensional space, and in recent years there has been some research into the extension of the path signature to multi-parameter fields (see e.g. Chouk/Gubinelli 14, Lee/Oberhauser (21 and 23)). In this talk I will propose an new extension of the path signature to two-parameter fields motivated by expansions of solutions to certain hyperbolic PDEs with multiplicative noise. The algebraic structure of this object turns out to be rather complicated and I will discuss our current understanding of the challenges with going from 1 to 2 parameters, and provide some interesting observations related to a Chen type relation and a Shuffle type relation. At last I will briefly discuss the universality of the 2D signature, providing a universal approximation theorem, and discuss some open problems. This talk is based on forthcoming joint work with Joscha Diehl, Kurusch Ebrahimi-Fard, and Samy Tindel, and is part of the Signatures for Images project for 2023/2024 at CAS.</p>
        <h2>Antonio Orvieto (ELLIS Institute Tübingen)</h2>
        <h3>Accurate and Efficient Processing of Long Sequences and Large Graphs without Attention</h3>
        <p>When applied to sequential data, transformers have an inherent challenge: their attention mechanism leads to quadratic complexity with respect to sequence length. This issue extends to graph transformers, where complexity scales quadratically with the number of nodes in the network. Today, we'll explore theoretically grounded alternatives to the attention mechanism that hinge on carefully parametrized linear recurrent neural networks. Unlike the more commonly known LSTMs and GRUs, linear RNNs are particularly GPU-efficient. This efficiency enables us to scale up the architecture, successfully study signal propagation, and achieve competitive performance. We'll present how, with a Linear Recurrent Unit (LRU) replacing attention, we can achieve state-of-the-art results on sequence modeling and graph data. This approach offers a promising direction for future research, especially in genetics, protein structure prediction, and audio/video processing and generation. Moreover, the architecture presents interesting connections with the randomized signature approach.</p>
        <h2>Carlos Amendola (TU Berlin)</h2>
        <h3>Convex Hulls of Curves: Volumes and Signatures</h3>
        <p>We study the use of path signatures to compute the volume of the convex hull of a curve. We present sufficient conditions for a curve so that the volume of its convex hull can be computed by such formulae. The canonical example is the classical moment curve, and our class of curves, which we call cyclic, includes other known classes such as d-order curves and curves with totally positive torsion. We also conjecture a necessary and sufficient condition on curves for the signature volume formula to hold. Joint work with Darrick Lee and Chiara Meroni.</p>
        <h2>Maud Lemercier (University of Oxford)</h2>
        <h3>A high-order numerical method for computing signature kernels</h3>
        <p>Signature kernels are at the core of several machine learning algorithms for analysing multivariate time series. The kernels of bounded variation paths, such as piecewise linear interpolations of time series data, are typically computed by solving a linear hyperbolic second-order PDE. However, this approach becomes considerably less practical for highly oscillatory inputs, due to significant time and memory complexities. To mitigate this issue, I will introduce a high-order method which involves replacing the original PDE, which has rapidly varying coefficients, with a system of coupled equations with piecewise constant coefficients. These coefficients are derived from the first few terms of the log-signatures of the input paths and can be computed efficiently using existing Python libraries.</p>
        <h2>Tim Seynnaeve (KU Leuven)</h2>
        <h3>Decomposing tensor spaces via path signatures</h3>
        <p>The signature of a path is a sequence of tensors whose entries are iterated integrals, playing a key role in stochastic analysis and applications. The set of all signature tensors at a particular level gives rise to the universal signature variety. We show that the parametrization of this variety induces a natural decomposition of the tensor space via representation theory, and connect this to the study of path invariants. We also reveal certain constraints that apply to the rank and symmetry of a signature tensor. This talk is based on joint work with Carlos Améndola, Francesco Galuppi, Ángel David Ríos Ortiz, and Pierpaola Santarsiero.</p>
        <h2>Stephane Chretien (University of Lyon 2)</h2>
        <h3>Signature estimation using Seigal's factorisation formula: anticoncentration and robustification</h3>
        <p>The theory of Signatures is a fast growing field which has demonstrated wide applicability to a large range of applications, from finance to health monitoring. Computing signatures often relies on the assumptions that the signal under study is not corrupted by noise, which is rarely the case in practice. In the present paper, we study the influence of noise on the computation of signature via the theory of anti-concentration. We then propose a median of means (MoM) approach to the estimation problem and give a bound on the estimation error using Rademacher complexity.</p>
        <h2>Nikolas Tapia (Weierstrass Institute)</h2>
        <h3>Stability of Deep Neural Networks via discrete rough paths</h3>
        <p>Using rough path techniques, we provide a priori estimates for the output of Deep Residual Neural Networks in terms of both the input data and the (trained) network weights. As trained network weights are typically very rough when seen as functions of the layer, we propose to derive stability bounds in terms of the total p-variation of trained weights for any p∈[1,3]. Unlike the C1-theory underlying the neural ODE literature, our estimates remain bounded even in the limiting case of weights behaving like Brownian motions, as suggested in (Cohen-Cont-Rossier-Xu, "Scaling Properties of Deep Residual Networks", 2021). Mathematically, we interpret residual neural network as solutions to (rough) difference equations, and analyse them based on recent results of discrete time signatures and rough path theory. Based on joint work with C. Bayer, S. Breneis and P. K. Friz.</p>
        <h2>Marco Rauscher (Technische Universität München)</h2>
        <h3>Shortest-path recovery from signature with an optimal control approach</h3>
        <p>In this talk, we consider the signature-to-path reconstruction problem from the control theoretic perspective. Namely, we design an optimal control problem whose solution leads to the minimal-length path that generates a given signature. In order to do that, we minimize a cost functional consisting of two competing terms, i.e., a weighted final-time cost combined with the L2-norm squared of the controls. Moreover, we can show that, by taking the limit to infinity of the parameter that tunes the final-time cost, the problem Γ converges to the problem of finding a sub-Riemannian geodesic connecting two signatures. Finally, we provide an alternative reformulation of the latter problem, which is particularly suitable for the numerical implementation.</p>
        <h2>Kurusch Ebrahimi-Fard (NTNU Norwegian University of Science and Technology)</h2>
        <h3>Log-signature of a surface holonomy</h3>
        <p>We will discuss the concept of log-signature in the context of surface holonomy. Based on joint work with I. Chevyrev, J. Diehl and N. Tapia.</p>
        <h1>Applications afternoon (Friday)</h1>
        <h2>Giovanni Ballarin (University of Mannheim)</h2>
        <h3>Memory of recurrent networks: Do we compute it right?</h3>
        <p>Numerical evaluations of the memory capacity (MC) of recurrent neural networks reported in the literature often contradict well-established theoretical bounds. In this paper, we study the case of linear echo state networks, for which the total memory capacity has been proven to be equal to the rank of the corresponding Kalman controllability matrix. We shed light on various reasons for the inaccurate numerical estimations of the memory, and we show that these issues, often overlooked in the recent literature, are of an exclusively numerical nature. More explicitly, we prove that when the Krylov structure of the linear MC is ignored, a gap between the theoretical MC and its empirical counterpart is introduced. As a solution, we develop robust numerical approaches by exploiting a result of MC neutrality with respect to the input mask matrix. Simulations show that the memory curves that are recovered using the proposed methods fully agree with the theory. Joint work with Lyudmila Grigoryeva and Juan-Pablo Ortega.</p>
        <h2>Remi Vaucher (University of Lyon 2 / Halias)</h2>
        <h3>Detecting anomalous dynamics in multivariate time series using signature methods</h3>
        <p>The signature of a path is a wonderful tool for extracting geometric information from a multivariate time series. These informations are particularly useful for prediction and classification. On our end, we are working on detecting abnormal changes in dynamics, especially applied to real data. In this presentation, we will discuss two methods: studying the distribution of well-chosen coefficients, as well as studying the topological structure underlying the set of channels induced by the signature transform.</p>
        <h2>Nozomi Sugiura (JAMSTEC - Japan Agency for Marine-Earth Science)</h2>
        <h3>Ocean Data Assimilation Focusing on Integral Quantities Characterizing Observation Profiles</h3>
        <p>An observation operator in data assimilation was formalized based on the signatures extracted from the integral quantities contained within observed vertical profiles in the ocean. A four-dimensional variational global ocean data assimilation system, founded on this observation operator, was developed and utilized to conduct preliminary data assimilation experiments over a ten-year assimilation window, comparing the proposed method, namely profile-by-profile matching, with the traditional method, namely point-by-point matching. The proposed method not only demonstrated a point-by-point skill comparable to the traditional method but also provided superior analysis fields in terms of profile shapes on the temperature-salinity plane. This is an indication of a well-balanced analysis field, in contrast to the traditional method, which can produce extremely poor relative errors for certain metrics.</p>        
        <h1>Spring school</h1>
        <h2>Valentin de Bortoli (ENS Ulm)</h2>
        <h3>Introduction to diffusion models</h3>
        <p>Denoising diffusion models are a new paradigm in the field of generative modeling in machine learning. In the past three years all recent state-of-the-art models have relied on this technique (image and music synthesis, video and 3D generation, protein modeling...). In this talk, I will introduce the basics of generative modeling and diffusion models from a statistical point of view. I will discuss some practical and theoretical properties of these models and present some open questions in the field.</p>
        <h2>Adeline Fermanian (Califrais) and Linus Bleistein (Inria Paris)</h2>
        <h3>Neural ODEs</h3>
        <p>Neural ODEs have emerged as a prominent model in the last years in a variety of areas in modern machine learning such as analyzing neural networks in the infinite depth limit, modeling time series or designing generative models. We start by discussing the links between ResNets and neural ODEs. We then present a few recent results on generalization, training and initialization of neural ODEs. Finally, we conclude the course by an in-depth review of continuous-time models for time series.</p>
      </div>
    </div>
  </body>
</html>
